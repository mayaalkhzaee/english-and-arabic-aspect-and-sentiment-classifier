{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15fc1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import fasttext\n",
    "    import fasttext.util\n",
    "    HAS_FASTTEXT = True\n",
    "except ImportError:\n",
    "    HAS_FASTTEXT = False\n",
    "    print(\"⚠ fasttext not installed. FastText baseline will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48af84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\ahmed\\Downloads\\aspect_extraction_and_classification\\final_dataset.jsonl\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "WINDOW_SIZE = 5\n",
    "DROP_CONFLICT = True\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee560c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_aspects(path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Each line in the JSONL file is a dict:\n",
    "    {\n",
    "      \"id\": int,\n",
    "      \"sentence\": str,\n",
    "      \"aspect_terms\": [\n",
    "          {\"term\": \"...\", \"polarity\": \"...\", \"from\": int, \"to\": int}\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    Returns a list of dicts in the same structure as load_semeval_xml().\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            sentence = obj[\"sentence\"]\n",
    "            aspects = []\n",
    "            for asp in obj.get(\"aspect_terms\", []):\n",
    "                try:\n",
    "                    aspects.append({\n",
    "                        \"term\": asp[\"term\"],\n",
    "                        \"polarity\": asp[\"polarity\"],\n",
    "                        \"from\": int(asp[\"from\"]),\n",
    "                        \"to\": int(asp[\"to\"]),\n",
    "                    })\n",
    "                except (KeyError, ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "            data.append({\n",
    "                \"id\": obj.get(\"id\"),\n",
    "                \"text\": sentence,\n",
    "                \"aspects\": aspects,\n",
    "            })\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bd385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Wrapper that chooses loader based on file extension.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".jsonl\", \".json\"]:\n",
    "        print(f\"Loading JSONL data from: {path}\")\n",
    "        return load_jsonl_aspects(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012983dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple text cleaning:\n",
    "    - strip\n",
    "    - lowercasing\n",
    "    - remove HTML-like tags\n",
    "    - normalize whitespace\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_token_window(text: str, start_char: int, end_char: int, window_size: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Convert char offsets into a token-level window of ±window_size words\n",
    "    around the aspect, and insert <ASP> ... </ASP> markers.\n",
    "I love <asp> battery </asp>\n",
    "    Uses the ORIGINAL text (not cleaned) to align with char offsets.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    starts = []\n",
    "\n",
    "    for m in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(m.group())\n",
    "        starts.append(m.start())\n",
    "\n",
    "    aspect_start_tok = None\n",
    "    aspect_end_tok = None\n",
    "\n",
    "    for i, (tok, s) in enumerate(zip(tokens, starts)):\n",
    "        e = s + len(tok)\n",
    "        # aspect start char falls into this token\n",
    "        if s <= start_char < e and aspect_start_tok is None:\n",
    "            aspect_start_tok = i\n",
    "        # aspect end char falls into this (or previous) token\n",
    "        if s < end_char <= e:\n",
    "            aspect_end_tok = i\n",
    "            break\n",
    "\n",
    "    if aspect_start_tok is None:\n",
    "        # fallback: full sentence if mapping fails\n",
    "        return text\n",
    "\n",
    "    if aspect_end_tok is None:\n",
    "        aspect_end_tok = aspect_start_tok\n",
    "\n",
    "    left = max(0, aspect_start_tok - window_size)\n",
    "    right = min(len(tokens), aspect_end_tok + 1 + window_size)\n",
    "\n",
    "    window_tokens = tokens[left:right]\n",
    "\n",
    "    rel_start = aspect_start_tok - left\n",
    "    rel_end = aspect_end_tok - left\n",
    "\n",
    "    # Insert tags around aspect span\n",
    "    window_tokens.insert(rel_start, \"<ASP>\")\n",
    "    window_tokens.insert(rel_end + 2, \"</ASP>\")  # +2 for the inserted <ASP>\n",
    "\n",
    "    return \" \".join(window_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d1c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_apc_dataset_with_windows(parsed_data: List[Dict],\n",
    "                                   window_size: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame with columns:\n",
    "      - sentence: cleaned full sentence\n",
    "      - sentence_raw: original sentence\n",
    "      - aspect: cleaned aspect term\n",
    "      - polarity: label (string)\n",
    "      - window: aspect-centered window with <ASP> tags (cleaned)\n",
    "      - input_full: aspect + [SEP] + full sentence (optional)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for item in parsed_data:\n",
    "        raw_text = item[\"text\"]\n",
    "        if not raw_text:\n",
    "            continue\n",
    "\n",
    "        for asp in item[\"aspects\"]:\n",
    "            term = asp[\"term\"]\n",
    "            pol = asp[\"polarity\"]\n",
    "            start = asp[\"from\"]\n",
    "            end = asp[\"to\"]\n",
    "\n",
    "            if term is None or pol is None:\n",
    "                continue\n",
    "\n",
    "            # Raw and cleaned forms\n",
    "            text_clean = clean_text(raw_text)\n",
    "            aspect_clean = clean_text(term)\n",
    "\n",
    "            # Window using original text for char offsets\n",
    "            window_raw = char_to_token_window(raw_text, start, end, window_size=window_size)\n",
    "            window_clean = clean_text(window_raw)\n",
    "\n",
    "            input_full = f\"{aspect_clean} [SEP] {text_clean}\"\n",
    "\n",
    "            rows.append({\n",
    "                \"sentence\": text_clean,\n",
    "                \"sentence_raw\": raw_text,\n",
    "                \"aspect\": aspect_clean,\n",
    "                \"polarity\": pol,\n",
    "                \"window\": window_clean,\n",
    "                \"input_full\": input_full,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac7ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorizer() -> FeatureUnion:\n",
    "    \"\"\"\n",
    "    Build a FeatureUnion of:\n",
    "      - word-level TF-IDF (1–2 grams)\n",
    "      - char-level TF-IDF (3–5 grams)\n",
    "\n",
    "    This will be used inside sklearn Pipelines so that\n",
    "    vectorization happens INSIDE cross-validation folds.\n",
    "    \"\"\"\n",
    "    word_tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=10000,\n",
    "        sublinear_tf=True,\n",
    "        analyzer=\"word\"\n",
    "    )\n",
    "\n",
    "    char_tfidf = TfidfVectorizer(\n",
    "        ngram_range=(3, 5),\n",
    "        max_features=20000,\n",
    "        sublinear_tf=True,\n",
    "        analyzer=\"char\"\n",
    "    )\n",
    "\n",
    "    vectorizer = FeatureUnion([\n",
    "        (\"word\", word_tfidf),\n",
    "        (\"char\", char_tfidf),\n",
    "    ])\n",
    "\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0890640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_pipelines(random_state: int = 42) -> Dict[str, Pipeline]:\n",
    "    \"\"\"\n",
    "    Return a dict of candidate pipelines (vectorizer + classifier)\n",
    "    for model selection using cross-validation.\n",
    "    \"\"\"\n",
    "    vectorizer = build_vectorizer()\n",
    "\n",
    "    pipelines = {\n",
    "        \"logreg\": Pipeline([\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                C=1.0,\n",
    "                class_weight=None,\n",
    "                random_state=random_state\n",
    "            ))\n",
    "        ]),\n",
    "        \"svm\": Pipeline([\n",
    "            (\"vectorizer\", build_vectorizer()),\n",
    "            (\"clf\", LinearSVC(\n",
    "                C=1.0,\n",
    "                random_state=random_state\n",
    "            ))\n",
    "        ]),\n",
    "        \"knn\": Pipeline([\n",
    "            (\"vectorizer\", build_vectorizer()),\n",
    "            (\"clf\", KNeighborsClassifier(\n",
    "                n_neighbors=5\n",
    "            ))\n",
    "        ]),\n",
    "        \"dt\": Pipeline([\n",
    "            (\"vectorizer\", build_vectorizer()),\n",
    "            (\"clf\", DecisionTreeClassifier(\n",
    "                max_depth=15,\n",
    "                random_state=random_state\n",
    "            ))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9242ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grid_for_model(model_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Define hyperparameter grid for the chosen best model.\n",
    "    Used in GridSearchCV.\n",
    "    \"\"\"\n",
    "    if model_name == \"logreg\":\n",
    "        return {\n",
    "            \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__solver\": [\"liblinear\", \"lbfgs\"],\n",
    "        }\n",
    "    elif model_name == \"svm\":\n",
    "        return {\n",
    "            \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "        }\n",
    "    elif model_name == \"knn\":\n",
    "        return {\n",
    "            \"clf__n_neighbors\": [3, 5, 7, 9],\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "        }\n",
    "    elif model_name == \"dt\":\n",
    "        return {\n",
    "            \"clf__max_depth\": [5, 10, 15, 20, None],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"No param grid defined for model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951d9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_cv(X_train, y_train, pipelines: Dict[str, Pipeline],\n",
    "                       n_folds: int = 5, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare candidate models using Stratified K-Fold cross-validation.\n",
    "    Returns a DataFrame with mean accuracy and std for each model.\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=n_folds,\n",
    "        shuffle=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for name, pipe in pipelines.items():\n",
    "        print(f\"\\n=== CV Evaluation: {name} ===\")\n",
    "        scores = cross_val_score(\n",
    "            pipe,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=\"accuracy\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        print(f\"Fold accuracies: {scores}\")\n",
    "        print(f\"Mean: {scores.mean():.4f} | Std: {scores.std():.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"model\": name,\n",
    "            \"mean_accuracy\": scores.mean(),\n",
    "            \"std_accuracy\": scores.std(),\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(\"mean_accuracy\", ascending=False)\n",
    "    print(\"\\n=== MODEL SELECTION (CV Results) ===\")\n",
    "    print(df_results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def run_grid_search_on_best(model_name: str,\n",
    "                            base_pipeline: Pipeline,\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            n_folds: int = 5,\n",
    "                            random_state: int = 42) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Run GridSearchCV on the chosen best model pipeline to fine-tune hyperparameters.\n",
    "    \"\"\"\n",
    "    param_grid = get_param_grid_for_model(model_name)\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=n_folds,\n",
    "        shuffle=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== GRID SEARCH on best model: {model_name} ===\")\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\\nBest CV accuracy: {grid.best_score_:.4f}\")\n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db131099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(best_model,\n",
    "                     X_test,\n",
    "                     y_test):\n",
    "    \"\"\"\n",
    "    Final evaluation on the held-out test set.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== FINAL TEST EVALUATION ===\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# OPTIONAL: FastText BASELINE\n",
    "# ==========================\n",
    "\n",
    "def load_fasttext_model(bin_path: str = \"cc.en.300.bin\"):\n",
    "    if not os.path.exists(bin_path):\n",
    "        print(\"Downloading FastText English model (cc.en.300.bin)...\")\n",
    "        fasttext.util.download_model(\"en\", if_exists=\"ignore\")  # creates cc.en.300.bin\n",
    "    model = fasttext.load_model(bin_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_fasttext_matrix(texts, ft_model):\n",
    "    vectors = []\n",
    "    for t in texts:\n",
    "        v = ft_model.get_sentence_vector(t)\n",
    "        vectors.append(v)\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "\n",
    "def fasttext_svm_baseline(X_train_texts, X_test_texts, y_train, y_test,\n",
    "                          random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Simple baseline: FastText embeddings + LinearSVC.\n",
    "    This is outside the main TF-IDF + grid search pipeline.\n",
    "    \"\"\"\n",
    "    if not HAS_FASTTEXT:\n",
    "        print(\"⚠ FastText not available. Skipping FastText baseline.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n=== FastText + SVM Baseline ===\")\n",
    "    ft_model = load_fasttext_model()\n",
    "    X_train_ft = build_fasttext_matrix(X_train_texts, ft_model)\n",
    "    X_test_ft = build_fasttext_matrix(X_test_texts, ft_model)\n",
    "\n",
    "    clf = LinearSVC(C=1.0, random_state=random_state)\n",
    "    clf.fit(X_train_ft, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test_ft)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy (FastText + SVM): {acc:.4f}\")\n",
    "    print(\"Classification report (FastText + SVM):\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60e7510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSONL data from: C:\\Users\\ahmed\\Downloads\\aspect_extraction_and_classification\\final_dataset.jsonl\n",
      "Total aspect instances after filtering: 20678\n",
      "                                            sentence  \\\n",
      "0  this sound track was beautiful! it paints the ...   \n",
      "1  this sound track was beautiful! it paints the ...   \n",
      "2  this sound track was beautiful! it paints the ...   \n",
      "3  this sound track was beautiful! it paints the ...   \n",
      "4  this sound track was beautiful! it paints the ...   \n",
      "\n",
      "                                        sentence_raw       aspect  polarity  \\\n",
      "0  This sound track was beautiful! It paints the ...  sound track  positive   \n",
      "1  This sound track was beautiful! It paints the ...        music  positive   \n",
      "2  This sound track was beautiful! It paints the ...      guitars  positive   \n",
      "3  This sound track was beautiful! It paints the ...   orchestras  positive   \n",
      "4  This sound track was beautiful! It paints the ...  keyboarding  negative   \n",
      "\n",
      "                                              window  \\\n",
      "0      this sound track was beautiful! it paints the   \n",
      "1  people who hate vid. game music! i have played...   \n",
      "2  a fresher step with grate guitars and soulful ...   \n",
      "3  with grate guitars and soulful orchestras. it ...   \n",
      "4  it backs away from crude keyboarding and takes...   \n",
      "\n",
      "                                          input_full  \n",
      "0  sound track [SEP] this sound track was beautif...  \n",
      "1  music [SEP] this sound track was beautiful! it...  \n",
      "2  guitars [SEP] this sound track was beautiful! ...  \n",
      "3  orchestras [SEP] this sound track was beautifu...  \n",
      "4  keyboarding [SEP] this sound track was beautif...  \n",
      "\n",
      "=== CV Evaluation: logreg ===\n",
      "Fold accuracies: [0.69598066 0.70565125 0.70435308 0.68440145 0.7025393 ]\n",
      "Mean: 0.6986 | Std: 0.0078\n",
      "\n",
      "=== CV Evaluation: svm ===\n",
      "Fold accuracies: [0.68117256 0.69628286 0.68681983 0.68349456 0.68833132]\n",
      "Mean: 0.6872 | Std: 0.0052\n",
      "\n",
      "=== CV Evaluation: knn ===\n",
      "Fold accuracies: [0.6237534  0.62224237 0.61638452 0.62484885 0.62877872]\n",
      "Mean: 0.6232 | Std: 0.0040\n",
      "\n",
      "=== CV Evaluation: dt ===\n",
      "Fold accuracies: [0.56119674 0.56029012 0.56106409 0.56227328 0.558948  ]\n",
      "Mean: 0.5608 | Std: 0.0011\n",
      "\n",
      "=== MODEL SELECTION (CV Results) ===\n",
      "    model  mean_accuracy  std_accuracy\n",
      "0  logreg       0.698585      0.007833\n",
      "1     svm       0.687220      0.005177\n",
      "2     knn       0.623202      0.004038\n",
      "3      dt       0.560754      0.001102\n",
      "\n",
      ">>> Selected best base model: logreg\n",
      "\n",
      "=== GRID SEARCH on best model: logreg ===\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\Downloads\\aspect_extraction_and_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best CV accuracy: 0.7000\n",
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "\n",
      "=== FINAL TEST EVALUATION ===\n",
      "Test Accuracy: 0.7108\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6944    0.8034    0.7449      1841\n",
      "     neutral     0.6027    0.0913    0.1586       482\n",
      "    positive     0.7331    0.7816    0.7565      1813\n",
      "\n",
      "    accuracy                         0.7108      4136\n",
      "   macro avg     0.6767    0.5587    0.5533      4136\n",
      "weighted avg     0.7006    0.7108    0.6817      4136\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1479   16  346]\n",
      " [ 268   44  170]\n",
      " [ 383   13 1417]]\n",
      "\n",
      "Final Test Accuracy of tuned logreg: 0.7108\n",
      "\n",
      "=== FastText + SVM Baseline ===\n",
      "Test Accuracy (FastText + SVM): 0.7065\n",
      "Classification report (FastText + SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6850    0.8267    0.7492      1841\n",
      "     neutral     0.7500    0.0373    0.0711       482\n",
      "    positive     0.7312    0.7623    0.7464      1813\n",
      "\n",
      "    accuracy                         0.7065      4136\n",
      "   macro avg     0.7221    0.5421    0.5223      4136\n",
      "weighted avg     0.7128    0.7065    0.6690      4136\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7064796905222437"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1. Load raw data\n",
    "parsed = load_data(DATA_PATH)\n",
    "\n",
    "# 2. Build aspect-based dataset with windows\n",
    "df = build_apc_dataset_with_windows(parsed, window_size=WINDOW_SIZE)\n",
    "\n",
    "if DROP_CONFLICT:\n",
    "    df = df[df[\"polarity\"] != \"conflict\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total aspect instances after filtering: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Prepare inputs and labels\n",
    "X_texts = df[\"window\"].values           # main input\n",
    "X_raw_for_ft = df[\"sentence_raw\"].values\n",
    "y = df[\"polarity\"].values\n",
    "\n",
    "# 4. Train-test split (held-out test for final evaluation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_texts,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train_raw, X_test_raw, _, _ = train_test_split(\n",
    "    X_raw_for_ft,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 5. Model selection via Stratified K-Fold CV\n",
    "candidate_pipelines = get_candidate_pipelines(random_state=RANDOM_STATE)\n",
    "cv_results = evaluate_models_cv(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    candidate_pipelines,\n",
    "    n_folds=N_FOLDS,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "best_model_name = cv_results.iloc[0][\"model\"]\n",
    "print(f\"\\n>>> Selected best base model: {best_model_name}\")\n",
    "\n",
    "best_base_pipeline = candidate_pipelines[best_model_name]\n",
    "\n",
    "# 6. Grid search on the selected best model\n",
    "grid = run_grid_search_on_best(\n",
    "    best_model_name,\n",
    "    best_base_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_folds=N_FOLDS,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# 7. Final evaluation on held-out test set\n",
    "test_acc = evaluate_on_test(\n",
    "    best_model,\n",
    "    X_test,\n",
    "    y_test\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy of tuned {best_model_name}: {test_acc:.4f}\")\n",
    "\n",
    "# 8. (Optional) FastText + SVM baseline\n",
    "fasttext_svm_baseline(\n",
    "    X_train_raw,\n",
    "    X_test_raw,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
